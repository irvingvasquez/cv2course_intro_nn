{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de una red multicapa\n",
    "\n",
    "En este ejercicio implementaremos el algoritmo de retropropagación dentro del descenso por gradiente para actualizar todos los pesos de la red durante varias épocas. Para entrenar la red usaremos el conjunto de datos de calificaciones que vimos previamente. Dicho conjunto tiene como vector de características la calificación del alumno en \"gre\", \"gpa\" y el \"ranking de su preparatoria\" y como valor objetivo esta su entrada a la universidad (como clase binaria).\n",
    "\n",
    "@juan1rving\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos paquetes y datos\n",
    "import numpy as np\n",
    "import nni.functions as functions\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "# Definiciones útiles\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features Summary Statistics:\n",
      "              gre         gpa      rank_1      rank_2      rank_3      rank_4\n",
      "count  360.000000  360.000000  360.000000  360.000000  360.000000  360.000000\n",
      "mean     0.019430    0.002307    0.158333    0.366667    0.302778    0.172222\n",
      "std      0.996139    0.988946    0.365561    0.482565    0.460099    0.378099\n",
      "min     -3.183094   -2.968993    0.000000    0.000000    0.000000    0.000000\n",
      "25%     -0.586063   -0.689498    0.000000    0.000000    0.000000    0.000000\n",
      "50%     -0.066657    0.026539    0.000000    0.000000    0.000000    0.000000\n",
      "75%      0.799020    0.736008    0.000000    1.000000    1.000000    0.000000\n",
      "max      1.837832    1.603135    1.000000    1.000000    1.000000    1.000000\n",
      "\n",
      "Target Value Counts:\n",
      "admit\n",
      "0    243\n",
      "1    117\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analizando los datos\n",
    "# Exploratory Data Analysis (EDA) for features\n",
    "print(\"\\nFeatures Summary Statistics:\")\n",
    "print(features.describe())\n",
    "\n",
    "# Checking the distribution of target values\n",
    "print(\"\\nTarget Value Counts:\")\n",
    "print(targets.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de unidades en la capa oculta\n",
    "n_hidden = 2 \n",
    "\n",
    "# Hiperparámetros\n",
    "epochs = 900\n",
    "learnrate = 0.001\n",
    "\n",
    "# Obtenemos el número de entradas (features) asi como el número de ejemplos (n_records)\n",
    "n_records, n_features = features.shape\n",
    "\n",
    "# Creamos las matrices de los pesos.\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.27631608246501976\n",
      "Train loss:  0.27631608246501976\n",
      "Train loss:  0.27631608246501976\n",
      "Train loss:  0.27631608246501976\n",
      "Train loss:  0.27631608246501976\n",
      "Train loss:  0.27631608246501976\n",
      "Train loss:  0.27631608246501976\n",
      "Train loss:  0.27631608246501976\n",
      "Train loss:  0.27631608246501976\n",
      "Train loss:  0.27631608246501976\n",
      "Exactitud sobre el conjunto de prueba: 0.300\n"
     ]
    }
   ],
   "source": [
    "#TODO (1 puntos): Completa el código para entrenar la red neuronal\n",
    "\n",
    "last_loss = None\n",
    "\n",
    "# Algoritmo de descenso por gradiente\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = None\n",
    "        hidden_output = None\n",
    "        output = None\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the error\n",
    "        error = None\n",
    "\n",
    "        # TODO: Calculate error term in output unit\n",
    "        output_error = None\n",
    "\n",
    "        # TODO: propagate errors to hidden layer\n",
    "        hidden_error = None\n",
    "\n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += 0\n",
    "        del_w_input_hidden += 0\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += 0\n",
    "    weights_hidden_output += 0\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = functions.sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = functions.sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Despues de entrenar la red, vamos a ver como se comporta en el conjunto de test\n",
    "hidden = functions.sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = functions.sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Exactitud sobre el conjunto de prueba: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (1 punto): Grafica la curva de aprendizaje (loss por época)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Responde a las siguientes preguntas (1 punto):\n",
    "\n",
    "- ¿La pedida de entrenamiento disminuye?\n",
    "\n",
    "- ¿Cuál es el mejor pérdida que alcanzaste? Menciona la cantidad de epocas y la tasa de aprendizaje usada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbad788490f55b163920bee5e9d5e0cba00db5905dc94f4bdbe0011e55bf465f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
