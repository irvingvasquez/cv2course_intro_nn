{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retropropagación a un paso\n",
    "\n",
    "La retropropagación nos brinda un mecanismo para entrenar redes de más de una capa. Su funcionamiento traslada el error en la predicción hacia atŕas usando la derivada parcial de dicho error con respecto de cada uno de los pesos. Es decir,\n",
    "\n",
    "$$\n",
    "\t\t\\frac{\\partial E}{\\partial w_{ho}} = \\frac{\\partial E}{\\partial h_o} \\frac{\\partial h_o}{\\partial w_{ho}}  =   \\frac{\\partial E}{\\partial h_o} y_h\n",
    "$$\n",
    "\n",
    "En este primer programa implementaremos el algoritmo de retropropagación para determinar los incrementos que debe tener cada capa de la red. En un siguiente ejercicio realizaremos todo el proceso de retropropagación.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/irvingvasquez/cv2course_intro_nn/blob/master/06_retropropagacion_base.ipynb)\n",
    "\n",
    "@juan1rving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos paquetes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos funciones útiles\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos una arquitectura de red 3x2x1\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pase frontal\n",
    "\n",
    "Definimos el comportamiento de la red en el pase frontal.\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(WX)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pase hacia atrás\n",
    "\n",
    "Calcula el cambio que deben de tener los pesos de acuerdo al error. Recordemos que para la capa de salida el término de error que se aplica es:\n",
    "\n",
    "$$\n",
    "\\delta_o = (y-\\hat{y})f'(h)\n",
    "$$\n",
    "\n",
    "Y para la capa oculta: \n",
    "\n",
    "$$\n",
    "\\delta_h = \\delta_o w_{h,o} f'(h_h)\n",
    "$$\n",
    "\n",
    "este último término de error es diferente para cada neurona con índice $h$ de la capa oculta. Es decir tendremos tantas h como neuronas tenga la capa oculta. Nota que aunque el ejercicio se puede resolver usando vectores e índices, lo implementaremos usando notación matricial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backwards pass\n",
    "## TODO: Calcula el error residual\n",
    "error = None\n",
    "\n",
    "# TODO: Calcula el término de error para la capa de salida\n",
    "del_err_output = None\n",
    "\n",
    "# TODO: Calcula el término de error para la capa oculta\n",
    "del_err_hidden = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que sabemos los términos de error que aplican en cada capa, procedemos a determinar los incrementos en cada capa.\n",
    "\n",
    "$$\n",
    "\\Delta w_{h,o} = \\Delta w_{h,o} + \\delta_o \\hat{y}_h \n",
    "$$\n",
    "\n",
    "$$\\Delta w_{i,h} = \\Delta w_{i,h} + \\delta_h x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcular el incremento en cada peso de la capa oculta a la salida\n",
    "delta_w_h_o = None\n",
    "\n",
    "# TODO: Calcular el incremento en la capa de entrada a la oculta\n",
    "delta_w_i_h = None\n",
    "\n",
    "print('Incremento de los pesos oculta a salida:')\n",
    "print(delta_w_h_o)\n",
    "\n",
    "print('Incremento de los pesos de entrada a oculta:')\n",
    "print(delta_w_i_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbad788490f55b163920bee5e9d5e0cba00db5905dc94f4bdbe0011e55bf465f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
