{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso por gradiente\n",
    "\n",
    "(Ejercicio, 3 puntos)\n",
    "\n",
    "En este notebook implementaremos un solo paso del método de descenso por gradiente en una neurona. El método es una técnica de optimización utilizada para encontrar el mínimo de una función de manera iterativa. En el contexto de redes neuronales, se utiliza para minimizar la función de costo, que mide el error entre las predicciones del modelo y los valores reales. El proceso comienza con una estimación inicial para los parámetros del modelo, y luego, en cada paso, ajusta estos parámetros en la dirección opuesta al gradiente de la función de costo, que indica la dirección de mayor aumento. La magnitud del ajuste en cada paso se determina por un parámetro llamado tasa de aprendizaje. El proceso se repite hasta alcanzar un mínimo local o hasta que el cambio en la función de costo entre iteraciones sea insignificante, indicando que el modelo ha convergido a una solución.\n",
    "\n",
    "![gradiente](files/gradient_descent_1n_notebook.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/irvingvasquez/cv2course_intro_nn/blob/master/03_descenso_unpaso.ipynb)\n",
    "\n",
    "@juan1rving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos paquetes\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la red neuronal\n",
    "\n",
    "Definiremos una red simple, una sola neurona. Es decir,\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(\\sum w_i \\cdot x_i + b)\n",
    "$$\n",
    "\n",
    "Y utilizaremos la función sigmoide:\n",
    "\n",
    "$$\n",
    "f(h) = \\sigma(h)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de activación\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivada de f\n",
    "def sigmoid_prime(h):\n",
    "    return sigmoid(h) * (1 - sigmoid(h))\n",
    "\n",
    "# clase Neurona\n",
    "class Neurona:\n",
    "    def __init__(self, W, b, activacion):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.activacion = activacion\n",
    "\n",
    "    def combinacion_lineal(self, X):\n",
    "        h = np.dot(self.W, X) + self.b\n",
    "        return h\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.combinacion_lineal(X)\n",
    "        return self.activacion(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Término de error\n",
    "\n",
    "Escribe una función que calcule el término de error\n",
    "\n",
    "$$\\delta= (y-\\hat{y})f' (h) = (y-\\hat{y})f' (\\sum_i w_i x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (1 punto): implementar el cálculo del término de error\n",
    "\n",
    "def error_term(y, x, neurona):\n",
    "    h = neurona.combinacion_lineal(x)\n",
    "    return (y - neurona.forward(x)) * sigmoid_prime(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremento\n",
    "\n",
    "Escribe una función para determinar el incremento a uno de los pesos\n",
    "$$\\Delta w_i= \\eta \\delta x_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (1 punto): implementar el cálculo del incremento\n",
    "def incremento(eta, error_term, X):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actualización\n",
    "\n",
    "Escribe una función para actualizar los pesos en la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (1 punto): implementar la actualización de los pesos\n",
    "def actualizacion(neurona, incremento_W, incremento_b):\n",
    "    neurona.W = None\n",
    "    neurona.b = None\n",
    "    return neurona\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificar funcionamiento\n",
    "\n",
    "A continuación implementemos una neurona de ejemplo y verificaremos que está funcionando almenos un paso del método de descenso por gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores de ejemplo\n",
    "tasa_aprendizaje = 2.0\n",
    "x = np.array([1, 1])\n",
    "y = 1.0\n",
    "\n",
    "# Valores iniciales de los pesos\n",
    "w = np.array([0.1,0.2])\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza las funciones previamente definidas para calcular lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Calcular la salida de la red\n",
    "neurona = Neurona(w, b, sigmoid)\n",
    "salida = None\n",
    "print('Salida:', salida)\n",
    "\n",
    "# ---- Descenso por gradiente ---------------\n",
    "\n",
    "# Calcula el término de error\n",
    "error_t = error_term(y, x, neurona)\n",
    "print('Término de error:', error_t)\n",
    "\n",
    "# Calcula el incremento de los pesos\n",
    "inc_w = incremento(tasa_aprendizaje, error_t, x)\n",
    "print('Incremento:', inc_w)\n",
    "\n",
    "# Calcula el incremento del sesgo\n",
    "inc_b = incremento(tasa_aprendizaje, error_t, 1)\n",
    "print('Incremento del sesgo:', inc_b)\n",
    "\n",
    "# Actualiza los pesos\n",
    "neurona = actualizacion(neurona, inc_w, inc_b)\n",
    "print('Pesos actualizados:', neurona.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 punto) Vuelve a realizar la inferencia y verifica que se está disminuyendo la pérdida."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
